{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/innobit/Desktop/document-parser/venv/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the doc\n",
    "def read_doc(dir):\n",
    "    file_loader = PyPDFDirectoryLoader(dir)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = read_doc('documents/')\n",
    "from langchain.schema import Document\n",
    "documents = [\n",
    "    {\n",
    "        \"stockcode\": \"A123\",\n",
    "        \"description\": \"High-quality Widget\",\n",
    "        \"price\": 19.99,\n",
    "        \"metadata\": {\n",
    "            \"stockcode\": \"A123\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"stockcode\": \"B456\",\n",
    "        \"description\": \"Durable Gadget\",\n",
    "        \"price\": 29.99,\n",
    "        \"metadata\": {\n",
    "            \"stockcode\": \"B456\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"stockcode\": \"C789\",\n",
    "        \"description\": \"Versatile Tool\",\n",
    "        \"price\": 39.99,\n",
    "        \"metadata\": {\n",
    "            \"stockcode\": \"C789\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"stockcode\": \"D012\",\n",
    "        \"description\": \"Reliable Appliance\",\n",
    "        \"price\": 49.99,\n",
    "        \"metadata\": {\n",
    "            \"stockcode\": \"D012\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"stockcode\": \"E345\",\n",
    "        \"description\": \"Innovative Device\",\n",
    "        \"price\": 59.99,\n",
    "        \"metadata\": {\n",
    "            \"stockcode\": \"E345\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "langchain_documents = []\n",
    "\n",
    "for item in documents:\n",
    "    # Create the page content by combining description and price\n",
    "    page_content = f\"{item['description']} - Price: ${item['price']}\"\n",
    "    \n",
    "    # Create a Document object\n",
    "    doc = Document(\n",
    "        page_content=page_content,\n",
    "        metadata={\n",
    "            \"stockcode\": item['stockcode'],\n",
    "            \"price\": item['price']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    langchain_documents.append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the docks into chunks\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    doc =text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = chunk_data(docs=langchain_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/innobit/Desktop/document-parser/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
       "), model_name='distiluse-base-multilingual-cased-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"distiluse-base-multilingual-cased-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = embeddings.embed_query(\"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector search db in pinecone\n",
    "pinecone_api_key = os.getenv('pinecone_api_key')\n",
    "pinecone_env = os.getenv('pinecone_environment')\n",
    "os.environ['PINECONE_API_KEY'] = os.getenv('pinecone_api_key')\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=pinecone_api_key,\n",
    ")\n",
    "\n",
    "index_name=os.getenv('pinecone_index_name')\n",
    "type(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc = Pinecone(\n",
    "#     api_key=\"ea8e8742-034e-423d-b89b-469e0068cf77\",\n",
    "# )\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # upsert the data to pinecone\n",
    "# for document in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(splits))]\n",
    "\n",
    "vector_store.add_documents(documents=splits, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(query=\"explain the story of Martin Luther King\",k=1)\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "\n",
    "connection_str = \"postgresql+psycopg2://postgres:test@localhost:5432/vector-db\"\n",
    "collection_name = \"products\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79191/552317497.py:1: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details aboutthe new implementation.\n",
      "  vectorstore = PGVector(\n",
      "/tmp/ipykernel_79191/552317497.py:1: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  vectorstore = PGVector(\n"
     ]
    }
   ],
   "source": [
    "vectorstore = PGVector(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection_string=connection_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = f\"pgvector/{collection_name}\"\n",
    "\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=connection_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_manager.create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'stockcode': 'A123', 'price': 19.99}, page_content='High-quality Widget - Price: $19.99'),\n",
       " Document(metadata={'stockcode': 'B456', 'price': 29.99}, page_content='Durable Gadget - Price: $29.99'),\n",
       " Document(metadata={'stockcode': 'C789', 'price': 39.99}, page_content='Versatile Tool - Price: $39.99'),\n",
       " Document(metadata={'stockcode': 'D012', 'price': 49.99}, page_content='Reliable Appliance - Price: $49.99'),\n",
       " Document(metadata={'stockcode': 'E345', 'price': 59.99}, page_content='Innovative Device - Price: $59.99')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 5, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index\n",
    "index(\n",
    "    splits,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"stockcode\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[1].page_content= \"modified\"\n",
    "del splits[0]\n",
    "splits.append(Document(page_content=\"new created\", metadata={\"stockcode\": \"A11101\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 2, 'num_updated': 0, 'num_skipped': 3, 'num_deleted': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index\n",
    "index(\n",
    "    splits,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"stockcode\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "docs[1].page_content = \"updated\"\n",
    "del docs[0]\n",
    "docs.append(Document(page_content=\"new content\", metadata={\"source\": \"important\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PGVector.from_documents(embedding=embeddings, documents=splits, collection_name=collection_name, connection_string=connection_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"explain the story of Wright brothers\"\n",
    "db.similarity_search_with_relevance_scores(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.schema import Document\n",
    "\n",
    "connection_str = \"postgresql+psycopg2://postgres:test@localhost:5432/vector-db\"\n",
    "collection_name = \"products\"\n",
    "\n",
    "# Initialize vectorstore\n",
    "vectorstore = PGVector.from_documents(\n",
    "    splits,\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection_string=connection_str\n",
    ")\n",
    "\n",
    "namespace = f\"pgvector/{collection_name}\"\n",
    "\n",
    "# Initialize record manager\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=connection_str\n",
    ")\n",
    "record_manager.create_schema()\n",
    "\n",
    "# Function to perform indexing\n",
    "def perform_indexing(documents):\n",
    "    index(\n",
    "        documents,\n",
    "        record_manager,\n",
    "        vectorstore,\n",
    "        cleanup=\"full\",  # Changed from \"incremental\" to \"full\"\n",
    "        source_id_key=\"stockcode\"\n",
    "    )\n",
    "\n",
    "# Initial indexing\n",
    "perform_indexing(splits)\n",
    "\n",
    "# Modify existing document\n",
    "splits[1].page_content = \"modified\"\n",
    "\n",
    "# Delete a document\n",
    "del splits[0]\n",
    "\n",
    "# Add new document\n",
    "splits.append(Document(page_content=\"new created\", metadata={\"stockcode\": \"A11101\"}))\n",
    "\n",
    "# Perform indexing again with updated documents\n",
    "perform_indexing(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.schema import Document\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "connection_str = \"postgresql+psycopg2://postgres:test@localhost:5432/vector-db\"\n",
    "collection_name = \"products\"\n",
    "\n",
    "# Initialize vectorstore\n",
    "vectorstore = PGVector(\n",
    "    collection_name=collection_name,\n",
    "    connection_string=connection_str,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "namespace = f\"pgvector/{collection_name}\"\n",
    "\n",
    "# Initialize record manager\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=connection_str\n",
    ")\n",
    "record_manager.create_schema()\n",
    "\n",
    "# Function to perform indexing\n",
    "def perform_indexing(documents):\n",
    "    # engine = create_engine(connection_str)\n",
    "    # with engine.connect() as conn:\n",
    "    #     # First, get the collection_id\n",
    "    #     result = conn.execute(text(f\"SELECT uuid FROM langchain_pg_collection WHERE name = '{collection_name}'\"))\n",
    "    #     collection_id = result.scalar()\n",
    "        \n",
    "    #     if collection_id:\n",
    "    #         # Delete existing records\n",
    "    #         conn.execute(text(f\"DELETE FROM langchain_pg_embedding WHERE collection_id = '{collection_id}'\"))\n",
    "    #         conn.execute(text(f\"DELETE FROM {namespace}\"))\n",
    "    #         conn.commit()\n",
    "    #     else:\n",
    "    #         print(f\"Collection '{collection_name}' not found. It will be created.\")\n",
    "\n",
    "    # Now, add all documents as new\n",
    "    # vectorstore.add_documents(documents)\n",
    "    \n",
    "    # Update the record manager\n",
    "    index(\n",
    "        documents,\n",
    "        record_manager,\n",
    "        vectorstore,\n",
    "        cleanup=\"incremental\",\n",
    "        source_id_key=\"stockcode\"\n",
    "    )\n",
    "\n",
    "# Initial indexing\n",
    "perform_indexing(splits)\n",
    "\n",
    "# Modify existing document\n",
    "splits[1].page_content = \"modified\"\n",
    "\n",
    "# Delete a document\n",
    "del splits[0]\n",
    "\n",
    "# Add new document\n",
    "splits.append(Document(page_content=\"new created\", metadata={\"stockcode\": \"A11101\"}))\n",
    "\n",
    "# Perform indexing again with updated documents\n",
    "perform_indexing(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
